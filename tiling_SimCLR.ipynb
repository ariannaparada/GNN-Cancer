{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tiling"
      ],
      "metadata": {
        "id": "DM4m17MysyuR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OInxcNMswnO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive, files\n",
        "import yaml\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dependencies\n",
        "!apt-get install -y openslide-tools\n",
        "!pip install openslide-python wandb\n",
        "!pip install torch torchvision torchaudio tqdm"
      ],
      "metadata": {
        "id": "mbef4ilMsyBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone repo\n",
        "!git clone https://github.com/vkola-lab/tmi2022.git\n",
        "%cd tmi2022"
      ],
      "metadata": {
        "id": "xSIam5bTs4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "id": "1fFDD1Tks6x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, subprocess, shlex\n",
        "\n",
        "wsi_dir = \"/content/drive/MyDrive/TCGA_WSIs\"\n",
        "output_patches = \"/content/drive/MyDrive/TCGA_WSIs/patches\"\n",
        "\n",
        "# all SVS (recursive)\n",
        "wsi_paths = glob.glob(os.path.join(wsi_dir, \"**\", \"*.svs\"), recursive=True)\n",
        "\n",
        "def stem(p):\n",
        "    return os.path.splitext(os.path.basename(p))[0]\n",
        "\n",
        "def already_tiled(p):\n",
        "    s = stem(p)\n",
        "    # typical output dir from tile_WSI.py (adjust if yours differs)\n",
        "    cand = os.path.join(output_patches, f\"{s}_files\")\n",
        "    if os.path.isdir(cand):\n",
        "        # optionally also ensure it has any files inside (skip half-written dirs)\n",
        "        for _root, _dirs, files in os.walk(cand):\n",
        "            if files:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# only the ones still missing\n",
        "to_tile = [p for p in wsi_paths if not already_tiled(p)]\n",
        "\n",
        "print(f\"SVS found: {len(wsi_paths)}\")\n",
        "print(f\"Already tiled: {len(wsi_paths) - len(to_tile)}\")\n",
        "print(f\"To tile now: {len(to_tile)}\")\n",
        "\n",
        "for wsi_path in to_tile:\n",
        "    print(\"Tiling\", wsi_path)\n",
        "    cmd = f'python src/tile_WSI.py -s 512 -e 0 -j 10 -B 50 -M 20 -o {shlex.quote(output_patches)} {shlex.quote(wsi_path)}'\n",
        "    ret = subprocess.call(cmd, shell=True)\n",
        "    if ret != 0:\n",
        "        print(\"FAILED:\", wsi_path)\n"
      ],
      "metadata": {
        "id": "zLYSYGgKs8rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many slides finished tiling?\n",
        "import os, glob\n",
        "patch_root = \"/content/drive/MyDrive/TCGA_WSIs/patches\"\n",
        "done = sorted(glob.glob(os.path.join(patch_root, \"*_files\")))\n",
        "len(done)"
      ],
      "metadata": {
        "id": "JqAFKup9tOMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "svs_root     = \"/content/drive/MyDrive/TCGA_WSIs\"        # where the .svs live (nested)\n",
        "patch_root   = \"/content/drive/MyDrive/TCGA_WSIs/patches\" # where tile folders are written\n",
        "\n",
        "# all slides (recursive)\n",
        "all_svs = sorted(glob.glob(os.path.join(svs_root, \"**\", \"*.svs\"), recursive=True))\n",
        "svs_base = {os.path.splitext(os.path.basename(p))[0] for p in all_svs}\n",
        "\n",
        "# all produced tile folders like <basename>_files\n",
        "tile_dirs = sorted(glob.glob(os.path.join(patch_root, \"*_files\")))\n",
        "tiled_base = {os.path.basename(d).replace(\"_files\",\"\") for d in tile_dirs}\n",
        "\n",
        "missing = sorted(svs_base - tiled_base)\n",
        "print(f\"SVS found: {len(all_svs)}\")\n",
        "print(f\"Tile folders: {len(tile_dirs)}\")\n",
        "print(f\"Missing (no *_files created): {len(missing)}\")"
      ],
      "metadata": {
        "id": "w9AQBKlLtPnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openslide import OpenSlide\n",
        "bad = []\n",
        "for p in all_svs:\n",
        "    try:\n",
        "        _ = OpenSlide(p)\n",
        "    except Exception as e:\n",
        "        bad.append((os.path.basename(p), str(e)))\n",
        "print(\"Unreadable slides:\", len(bad))"
      ],
      "metadata": {
        "id": "ZNgm37mbtRkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsi_dir = \"/content/drive/MyDrive/TCGA_WSIs\"\n",
        "output_patches = \"/content/drive/MyDrive/TCGA_WSIs/patches\"\n",
        "\n",
        "os.listdir(output_patches)"
      ],
      "metadata": {
        "id": "xcopmjDFtZF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "%cd /content/tmi2022/feature_extractor\n",
        "\n",
        "# Adjust path if needed\n",
        "#image_paths = glob.glob(\"/content/drive/MyDrive/TCGA_WSIs/patches/*/*/*.jpeg\", recursive=True)\n",
        "\n",
        "# Convert to DataFrame\n",
        "#df = pd.DataFrame(image_paths)\n",
        "\n",
        "# Save to CSV\n",
        "#df.to_csv(\"patches.csv\", index=False, header=False)\n",
        "\n",
        "!find /content/drive/MyDrive/TCGA_WSIs/patches -type f -iname '*.jpeg' > patches.csv\n"
      ],
      "metadata": {
        "id": "YNeE3wfAtbVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "%cd /content/tmi2022\n",
        "\n",
        "cfg_path = \"/content/tmi2022/feature_extractor/config.yaml\"\n",
        "config = {\n",
        "    'batch_size': 64,\n",
        "    'epochs': 20,\n",
        "    'eval_every_n_epochs': 1,\n",
        "    'fine_tune_from': '',\n",
        "    'log_every_n_steps': 25,\n",
        "    'weight_decay': \"1e-5\",\n",
        "    'fp16_precision': False,\n",
        "    'n_gpu': 1,\n",
        "    'gpu_ids': [0],\n",
        "    'model': {\n",
        "        'out_dim': 512,\n",
        "        'base_model': \"resnet50\"\n",
        "    },\n",
        "    'dataset': {\n",
        "        's': 1,\n",
        "        'input_shape': \"(224, 224, 3)\",   # â† keep as STRING for their eval(...)\n",
        "        'num_workers': 10,\n",
        "        'valid_size': 0.1\n",
        "    },\n",
        "    'loss': {\n",
        "        'temperature': 0.5,\n",
        "        'use_cosine_similarity': True\n",
        "    }\n",
        "    # you can also add 'lr': \"1e-5\" if somewhere else they eval lr from config\n",
        "}\n",
        "\n",
        "with open(cfg_path, 'w') as f:\n",
        "    yaml.dump(config, f, sort_keys=False)\n",
        "\n",
        "print(open(cfg_path).read())\n"
      ],
      "metadata": {
        "id": "zo5df2kntetG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml, pathlib\n",
        "cfg_path = pathlib.Path(\"/content/tmi2022/feature_extractor/config.yaml\")\n",
        "cfg = yaml.safe_load(open(cfg_path))\n",
        "\n",
        "cfg['batch_size'] = 64          # try 64; if OOM persists, try 32\n",
        "cfg['dataset']['input_shape'] = \"(224, 224, 3)\"  # keep as string for their eval()\n",
        "cfg['dataset']['num_workers'] = 4                # lighter on RAM\n",
        "cfg['fp16_precision'] = True                     # we'll enable native AMP below\n",
        "cfg['weight_decay'] = \"1e-5\"                     # keep as string due to eval() in repo\n",
        "\n",
        "yaml.dump(cfg, open(cfg_path, \"w\"), sort_keys=False)\n",
        "print(open(cfg_path).read())\n"
      ],
      "metadata": {
        "id": "HO7UEufwtgIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, imghdr, time\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageFile\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "%cd /content/tmi2022/feature_extractor\n",
        "\n",
        "# --- settings ---\n",
        "CSV_IN   = \"patches.csv\"\n",
        "CSV_OUT  = \"all_patches.csv\"\n",
        "CHECKPOINT_EVERY = 10000   # write partial results every N files\n",
        "MAX_WORKERS = 16           # Colab often benefits from 8-16 for IO-bound tasks\n",
        "MIN_BYTES = 1024           # skip tiny files\n",
        "EXT_OK = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".webp\"}\n",
        "\n",
        "# Make PIL less picky about large images; allow truncated loads\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "# --- load & clean paths ---\n",
        "df = pd.read_csv(CSV_IN, header=None)\n",
        "print(f\"Original entries: {len(df)}\")\n",
        "\n",
        "# Normalize strings & drop obvious dups\n",
        "paths = (\n",
        "    df[0].astype(str).str.strip()\n",
        "      .str.replace(r\"^file://\", \"\", regex=True)\n",
        "      .dropna().drop_duplicates()\n",
        "      .tolist()\n",
        ")\n",
        "print(f\"Unique paths after basic cleanup: {len(paths)}\")\n",
        "\n",
        "def cheap_prefilter(p: str) -> bool:\n",
        "    \"\"\"Fast checks before touching PIL.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(p): return False\n",
        "        if os.path.getsize(p) < MIN_BYTES: return False\n",
        "        ext = os.path.splitext(p)[1].lower()\n",
        "        if ext and ext not in EXT_OK: return False\n",
        "        # Quick signature sniff (very fast; not perfect but good filter)\n",
        "        kind = imghdr.what(p)\n",
        "        if kind is None:\n",
        "            # Some TIFF/WEBP may fail imghdr; keep them for PIL step if extension ok\n",
        "            if ext in {\".tif\", \".tiff\", \".webp\"}:\n",
        "                return True\n",
        "            return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def deep_check_with_pil(p: str) -> bool:\n",
        "    \"\"\"Heavier verification: only run for items passing the prefilter.\"\"\"\n",
        "    try:\n",
        "        with Image.open(p) as img:\n",
        "            img.verify()  # integrity check without decoding pixels\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def validate_path(p: str) -> str | None:\n",
        "    if not cheap_prefilter(p):\n",
        "        return None\n",
        "    return p if deep_check_with_pil(p) else None\n",
        "\n",
        "valid_paths = []\n",
        "start = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    futures = {ex.submit(validate_path, p): p for p in paths}\n",
        "    for i, f in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"Validating\")):\n",
        "        res = f.result()\n",
        "        if res is not None:\n",
        "            valid_paths.append(res)\n",
        "        # checkpoint occasionally\n",
        "        if (i + 1) % CHECKPOINT_EVERY == 0:\n",
        "            pd.DataFrame(valid_paths).to_csv(CSV_OUT + \".part\", index=False, header=False)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"Validated in {elapsed/60:.1f} min\")\n",
        "\n",
        "# Save final\n",
        "df_valid = pd.DataFrame(valid_paths)\n",
        "df_valid.to_csv(CSV_OUT, index=False, header=False)\n",
        "print(f\"Valid images: {len(df_valid)}\")\n",
        "print(f\"Removed: {len(df) - len(df_valid)} bad entries\")\n",
        "print(f\"Filtered CSV saved to {CSV_OUT}\")\n"
      ],
      "metadata": {
        "id": "Zl0f2AhHthke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tmi2022/feature_extractor\n",
        "\n",
        "import yaml\n",
        "from data_aug.dataset_wrapper import DataSetWrapper\n",
        "\n",
        "cfg = yaml.load(open(\"config.yaml\",\"r\"), Loader=yaml.FullLoader)\n",
        "train_loader, _ = DataSetWrapper(cfg['batch_size'], **cfg['dataset']).get_data_loaders()\n",
        "print(\"Steps per epoch:\", len(train_loader))\n"
      ],
      "metadata": {
        "id": "1zhGQjr8tjfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run FE\n",
        "%cd /content/tmi2022/feature_extractor\n",
        "!python run.py\n",
        "\n",
        "# for pretrained model\n",
        "# !wget -O model.pth https://example.com/path/to/pretrained_model.pth\n",
        "\n"
      ],
      "metadata": {
        "id": "2TvfylbCtlnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_losses_separate.py\n",
        "import os, glob\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Point this to EITHER the specific run dir (that contains events.out.tfevents.*)\n",
        "# OR the parent \"runs\" directory. The helper will auto-pick the latest event dir.\n",
        "LOG_PATH = \"/content/tmi2022/feature_extractor/runs\"\n",
        "\n",
        "def find_event_dir(path):\n",
        "    \"\"\"Return a directory that directly contains TensorBoard event files.\"\"\"\n",
        "    def has_events(p):\n",
        "        return bool(glob.glob(os.path.join(p, \"events.out.tfevents.*\")))\n",
        "    if os.path.isdir(path) and has_events(path):\n",
        "        return path\n",
        "    candidates = [d for d in glob.glob(os.path.join(path, \"*\"))\n",
        "                  if os.path.isdir(d) and has_events(d)]\n",
        "    if not candidates:\n",
        "        raise SystemExit(f\"No TensorBoard event files found under: {path}\")\n",
        "    return max(candidates, key=os.path.getmtime)\n",
        "\n",
        "def get_series(ea, epoch_tag, step_tag):\n",
        "    # prefer epoch-level; fall back to step-level\n",
        "    try:\n",
        "        epoch_data = ea.Scalars(epoch_tag)\n",
        "    except KeyError:\n",
        "        epoch_data = []\n",
        "    if epoch_data:\n",
        "        return [s.step for s in epoch_data], [s.value for s in epoch_data], \"Epoch\"\n",
        "    try:\n",
        "        step_data = ea.Scalars(step_tag)\n",
        "    except KeyError:\n",
        "        step_data = []\n",
        "    return ([s.step for s in step_data], [s.value for s in step_data], \"Step\")\n",
        "\n",
        "run_dir = find_event_dir(LOG_PATH)\n",
        "print(\"Using event dir:\", run_dir)\n",
        "\n",
        "ea = EventAccumulator(run_dir)\n",
        "ea.Reload()\n",
        "print(\"Available scalar tags:\", ea.Tags().get(\"scalars\", []))\n",
        "\n",
        "# --- fetch series ---\n",
        "tr_x, tr_y, tr_xlabel = get_series(ea, \"epoch_train_loss\", \"train_loss\")\n",
        "va_x, va_y, va_xlabel = get_series(ea, \"epoch_val_loss\", \"validation_loss\")\n",
        "\n",
        "if not tr_y and not va_y:\n",
        "    raise SystemExit(\"No train/val loss scalars found. Check tags printed above.\")\n",
        "\n",
        "# Save under run_dir/checkpoints (create if missing)\n",
        "ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "# --- TRAIN plot (single series) ---\n",
        "if tr_y:\n",
        "    plt.figure()\n",
        "    plt.plot(tr_x, tr_y)\n",
        "    plt.xlabel(tr_xlabel)\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"SimCLR Training Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    out_train = os.path.join(ckpt_dir, \"train_loss_curve.png\")\n",
        "    plt.savefig(out_train, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Saved:\", out_train)\n",
        "else:\n",
        "    print(\"No training loss scalars found.\")\n",
        "\n",
        "# --- VALIDATION plot (single series) ---\n",
        "if va_y:\n",
        "    plt.figure()\n",
        "    plt.plot(va_x, va_y)\n",
        "    plt.xlabel(va_xlabel)\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"SimCLR Validation Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    out_val = os.path.join(ckpt_dir, \"val_loss_curve.png\")\n",
        "    plt.savefig(out_val, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Saved:\", out_val)\n",
        "else:\n",
        "    print(\"No validation loss scalars found.\")\n"
      ],
      "metadata": {
        "id": "juMC3_C_tnAm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}